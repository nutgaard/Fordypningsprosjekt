\section{The process}
\subsection{Extract}
We must extract the data from the FS system(initially this will be the only source of data). We do not still know how this can be done, but have seen examples of student data being extracted through CSV files. 
Since the structure of the data and the accessibility is still unknown it is impossible to explain any details around this process.
Though the process may simply be viewed as loading the data from a given resource into memory, and thus preparing it for the transformation process.

\bigskip\noindent
In order to achieve a higher accuracy we should also investigate 
the possibility of extracting data from It's learning to augment the FS data.

\subsection{Transform}
In order to get the data persisted in the data warehouse we need to transform the data.
The transformation process can in most cases be done automatically given a set of assumptions and rules.
These assumptions and rules may, as an example, describe what defines a student dropout. 

\bigskip\noindent
A second step of the transformation phase is anonymization. 
This is normally not part of the standard process, but due to the possibly sensitive nature of our data this is a necessary step.
We need to anonymize the data by removing obvious fields such as name, address, student id and so forth. 
Student can however still be identified by their grades, hence removing unique entries from the data set may be an option.
K-anonymization or common clustering techniques may help us achieve this goal while maintaining the classification structure and accuracy.

\subsection{Load}
The transformed data from the previous phase is persisted into the datawarehouse. 
In this specific case this data may include courses, grades, exams, number of tries for each course, 
and any reason a student may have for not completing a course or failing an exam.

\bigskip\noindent
We propose an exam centric data model, where exam entries are treated as the \textit{fact table}. 
The reason behind choosing an exam centric model instead of a student centric model comes from the
need to keep the dimensions of each table at a fixed size. Something that would not be possible 
with the student centric model due variations in courses and number of tries at each course.
\image{UML/examCentricDataModel.png}{\columnwidth}{The general structure of an exam centric data model}{fig:examCentricDataModel}

The proposed data model still yields the same flexibility as a student centric model would. 
But may see some performance penalties due to the extra \textit{table joins} that are needed to consolidate the data.
This performance decrease can however be mitigated by the use of \textit{materialized view}.
For example a view over number of tries each student has on each of his passed courses.

\subsection{Prepare}
Although a lot of data juggling is done during the transformation phase, it does not specialize the data towards its final use. 
The prepare phase aims to close the final gap between the persisted data structure and the optimal data structure for the classifiers.
Depending on the classifier to be used several approaches may be of relevance, especially balancing of training data to be used.
The balancing is there because of the skewed probability distribution of dropout given a random student attending the school.
Thus students who complete their study will dominate the classifier and yield poor accuracy for cases where students quit.

\bigskip\noindent
Another point of attention is discretization of values. 
This is due to the specific needs of different classifiers, where some does not handle continuous values well. 
One attribute that may be interesting to discretize is the age of a given student, 
since age group may be a better indicator of a students performance than the age itself. 
Other aggregated values, as average grade, may also gain accuracy by discretization.

\bigskip\noindent
Another factor in choosing Weka as the framework is its built-in features for both of the techniques mentioned above. 

\subsection{Analyze}
As seen in chapter~\ref{ch:related} the most efficient data mining techniques for educational data are the simple and explanatory ones,
specifically decision trees and rule inducing algorithms.
Artificial neural networks have also been used with varying success but should be considered.

\subsection{Visualize}	
Visualization is the step of communicating the information found clearly through graphical means. 
This is useful because humans are exceptional at extracting structures from pictures compared to computers. 
Integration between visualization and data mining is called visual data mining.
In this proposed system we have chosen a loose integration of visual data mining where output from the analyze step is used as input for the visualization step.
Other possible integrations are no integration, where you only use one or full integration, where both methods are applied in parallel.
Visualization techniques that might be interesting include histogram, pie chart, scatter plots, line graphs, icon based methods and pixel based methods.
The choice is largely dependent on the number of dimensions and whether variables are related to each other or not.

\bigskip\noindent
This is helpful for understanding which attributes are most significant for student dropout. 
Not only is it helpful but many mining techniques require user intervention at different stages and visualization is starting to be used to support the decision process~\cite{1207445}.
Overall visual data mining looks promising and this should probably be expanded on in the future.
