\subsubsection{Artificial Neural Network}
	Another technique from the discipline of artifical intelligence is artificial neural networks (\textbf{ANN}).
	Though ANNs may be used for a variaty of task, we will focus on what ANNs are and how they may be used as
	a classifier. 
	
	\bigskip\noindent
	First of, a general introduction to ANN is in order. ANNs are computation models that attempt to capture
	the behaviour and adaptive features of the brain by modeling it.
	An ANN therefore consists of a set of neurons, the computational unit, and their synapses, the relationship between neurons (see figure~\ref{fig:ann}).
	
	\image{images/ann.png}{0.5\columnwidth}{Structure of artifical neural network}{fig:ann}
	
	\bigskip\noindent
	Neurons is characterized by a set of parameters, including the connection strength, threshold, and an activation function
	(see figure~\ref{fig:neuron} and equations~\ref{eq:annsummation}-\ref{eq:annsigmoid}). 
	This is however just the most common and basic implementation of a neural network. 
	The full discussion and other extended versions of neural networks are beyond the scope of this paper.
	
	\begin{figure}
		\begin{align}
			net_j &= \sum_{i=1}^n x_i*w_{ij}\label{eq:annsummation}\\
			o(j) &= \varphi (net_j - \theta_j)\label{eq:annthreshold}\\
			\varphi(x) &= kx\label{eq:annlinear}\\
			\varphi(x) &= \begin{cases}
					1, & \text{if $x > \theta_j$}.\\
					0, & \text{otherwise.}
				\end{cases}\label{eq:annbinary}\\
			\varphi(x) &= \begin{cases}
					1, & \text{if $x > \theta_j$}.\\
					-1, & \text{otherwise.}
				\end{cases}\label{eq:annbipolar}\\
			\varphi(x) &= \frac{1}{1+e^{-kx}}\label{eq:annsigmoid}
		\end{align}
	\end{figure}
	
	\image{images/neuron.png}{\columnwidth}{The inner workings of a artifical neuron}{fig:neuron}
	
	\bigskip\noindent
	Training of neural networks consists of three known paradigms, supervised, unsupervised and reinforced. 
	Where it is supervised learning that is of interest to us in the context of a classification task. 
	Unsupervised learning could however be used as a clustering technique, 
	while reinforce learning often is modeled as a Markov decision process and therefore fits the task of sequential decision making. \cite{bioAI}
	
	\bigskip\noindent
	Within the supervised learning paradigm there exists several algorithms. 
	Most commonly known are perhaps \textit{Widrow-Hoff} rule (delta rule)\cite{widrowhoff}, and the later extended version \textit{backpropagation of error}\cite{rumelhart1986learning}\cite{rumelhart1986parallel}. 
	The core of the backpropagation algorithm consists of calculating how each of the nodes contribute to the final answers' error, 
	and adjusting the weights of those nodes according to a preset learningrate. 
	What this algorithms in a sense does it to preform a gradient descent search of the error function at each neuron.
	Thus making the neural network as a whole converge towards the target function. 
	
	\bigskip\noindent
	One disadvantage of the ANN approach is that the resulting network in most cases yields little to none addtional information.
	This stands in contrast to the CART based classifers, where the final classifier (tree) in most cases give addtional interpretable information. 
	One example of this would be to look at the root node of a tree generated by one of the entropy based algorithms.
	The root node would in those cases represent the attribute which best classified the trainingset, 
	and thus the attribute which has be greatest influence of the target classification. 
	This additional analysis would be extremely tedious to perform using artifical neural networks due to their highly connected nature.
	Research into rule extraction from trained neural network has been done with satisfying result, 
	it still requires an extra step of postprocessing to elict the rules.~\cite{augastarule}
	
	
	
	
	
	